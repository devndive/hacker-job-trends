{"by":"CaliforniaKarl","id":14690729,"parent":14688684,"text":"Stanford Research Computing Center | HPC System Administrator | Stanford, CA | ONSITE<p>Hello!<p>The Stanford Research Computing Center (SRCC, <a href=\"http:&#x2F;&#x2F;srcc.stanford.edu\" rel=\"nofollow\">http:&#x2F;&#x2F;srcc.stanford.edu</a>) is a join effort of the Dean of Research and University IT.  We are looking for someone to join our team as an HPC System Administrator.<p>The SRCC provides lots of services to the Stanford community (which includes non-Stanford researchers collaborating on Stanford research):  We maintain several large compute environments (like Sherlock—yes, the Sherlock mentioned recently on Silicon Valley) for various uses.  We offer consultation services, helping users get the most from our environments.  We offer workshops and training, on topics relevant to the HPC community.  We also offer architecture, sysadmin, and development services to labs.  We also offer hosting in the SRCF, our purpose-built research data center.<p>As HPC System Administrator, you&#x27;re going to be responsible for one of our larger compute environments.  You&#x27;ll also be one of the points of contact for several labs on campus (I&#x27;m the other point of contact).  All of the systems that you&#x27;ll be interacting with will be running Linux, typically either CentOS 6&#x2F;7 or Ubuntu LTS.<p>Even though I summarized the duties in one paragraph, you should expect to have various weird requests come up.  For example, in the 2 years I&#x27;ve been here, I&#x27;ve…<p>• Automated the DNA post-sequencing workflow for a lab.<p>• Helped secure a number of systems which hold HIPAA data.<p>• Acted as the SRCC representative at a number of office hours, where community members come for in-person support.<p>• Created the foundational Puppet code for the next iteration of our FarmShare cluster.<p>• Created a one-sheet flyer (with Inkscape and Scribus) that is now used by our manager, and by our faculty liasons.<p>• Was primarily responsible for our table at a recent open-house.<p>The first three items are things you should expect to do, and the remaining items are the kinds of things that you can step up and do when the need arises.<p>We use tons of different technologies.  We use Puppet in some places, and Rocks in others.  We use technologies (like Infiniband, Lustre, and GPFS) and software (like SLURM and MPI) that you may not have used before, and we also use stuff (like Linux, Git, Apache, Kerberos, and LDAP) that you likely have used before.<p>This position is on-site, on Stanford&#x27;s campus.  We want people to commute as much as possible, so we&#x27;ll pay for your Caltrain; if you live in Santa Clara county, we&#x27;ll also pay for VTA.  Our primary data center is on the SLAC campus, but we have our own van (it&#x27;s pretty cool), so getting around is no problem.<p>You can read the job posting at [1].  You should also feel free to reach out if you have questions!  My email address is in my profile.<p>[1]: <a href=\"https:&#x2F;&#x2F;stanford.taleo.net&#x2F;careersection&#x2F;jobdetail.ftl?job=75233&amp;lang=en\" rel=\"nofollow\">https:&#x2F;&#x2F;stanford.taleo.net&#x2F;careersection&#x2F;jobdetail.ftl?job=7...</a>","time":1499110112,"type":"comment"}