{"by":"LisaG","id":3653337,"parent":3652041,"text":"San Francisco CA (remote okay)<p>Crawl Engineer and Big Data Enthusiast<p>We're looking for someone enthusiastic about open source, net neutrality, open data and keeping the web truly open. Common Crawl is dedicated to building and maintaining an open repository of web crawl data in order to enable a new wave of innovation, education and research. If you're looking to do work that matters, come join us!<p>We're set to do amazing things this year, and there is no better place to hone your big data skills than helping us manage and process our 50 TB corpus. Plus, you'll be working within a passionate community and have the chance to interface with plenty of talented researchers, educators, startup folks, and an incredible advisory board.<p>Responsibilities<p><pre><code>  * Improve the stability, scaling, and visibility of our distributed web crawler\n  * Use, improve, and extend our post-crawl, Hadoop-based data processing pipeline \n  * Design and build mechanism for specification and execution of custom crawls\n</code></pre>\nDesired Skills &#38; Experience<p><pre><code>  * You can architect and code for a system with tens of billions of documents\n  * Strong coding ability in Java \n  * Strong coding skills in at least one scripting language (Python, Ruby, Perl...)\n  * In-depth knowledge of HTTP and are familiar with web crawlers\n  * You have development and administrative experience with Hadoop and HDFS\n  * Ops experience with Linux or other UNIX\n  * Some familiarity with AWS, including one or more of EC2, S3, EBS, and EMR\n  * Like to build useful, thorough documentation of code and systems\n  * Self-starter wiling to take ownership of projects\n</code></pre>\n<a href=\"http://www.commoncrawl.org\" rel=\"nofollow\">http://www.commoncrawl.org</a>","time":1330626558,"type":"comment"}