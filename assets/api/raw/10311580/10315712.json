{"by":"Smerity","id":10315712,"parent":10311580,"text":"Common Crawl (<a href=\"http:&#x2F;&#x2F;commoncrawl.org&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;commoncrawl.org&#x2F;</a>)<p>Position: Crawl Engineer &#x2F; Data Scientist<p>Location: SF or Remote<p>Email: jobs@commoncrawl.org<p>Common Crawl is the non-profit organization that builds and maintains the single largest publicly accessible dataset of the world&#x27;s knowledge, encompassing petabytes of web crawl data. Any can download and use the data for free and we&#x27;ve been used for a wide variety of purposes.<p>As the crawl engineer, you&#x27;ll run a crawl that spans hundreds of millions of domains and billions of pages each month. You&#x27;ll command a fleet of machines on AWS that use Nutch to capture the web data and then Hadoop to turn it into a better structured dataset for others to use. This is a rewarding role as you&#x27;re really giving back to the open data community :)<p>Requirements:<p><pre><code>  - Fluent in Java (Nutch and Hadoop are core to our mission)\n  - Familiarity with the JVM big data ecosystem (Hadoop, HDFS, ...)\n  - Knowledge the Amazon Web Services (AWS) ecosystem\n  - Experience with Python\n  - Basic command line Unix knowledge\n  - BS Computer Science or equivalent work experience\n</code></pre>\nFull details: <a href=\"http:&#x2F;&#x2F;commoncrawl.org&#x2F;jobs&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;commoncrawl.org&#x2F;jobs&#x2F;</a>","time":1443744221,"type":"comment"}